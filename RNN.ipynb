{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e29c869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de488d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/darshan39/Downloads/100_Unique_QA_Dataset.csv\")\n",
    "# normalize column names and fill missing question/answer cells\n",
    "df.columns = df.columns.str.strip()\n",
    "if 'Question' in df.columns and 'Answer' in df.columns:\n",
    "    df[['Question', 'Answer']] = df[['Question', 'Answer']].fillna('')\n",
    "else:\n",
    "    # show columns for debugging if expected columns not present\n",
    "    print('Columns in dataframe:', df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab9a516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of Germany?</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
       "      <td>Harper-Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>Jupiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the boiling point of water in Celsius?</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question      answer\n",
       "0                   What is the capital of France?       Paris\n",
       "1                  What is the capital of Germany?      Berlin\n",
       "2               Who wrote 'To Kill a Mockingbird'?  Harper-Lee\n",
       "3  What is the largest planet in our solar system?     Jupiter\n",
       "4   What is the boiling point of water in Celsius?         100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e502941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust tokenizer that tolerates non-string inputs\n",
    "import re\n",
    "def tokenize(text):\n",
    "    # ensure we work with a string and remove common punctuation\n",
    "    if text is None:\n",
    "        text = ''\n",
    "    text = str(text)\n",
    "    # replace question marks/apostrophes with space and remove other punctuation\n",
    "    text = text.replace('?', ' ').replace(\"'\", ' ')\n",
    "    text = re.sub(r'[^0-9a-zA-Z\\s]', ' ', text)\n",
    "    return text.lower().strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "297ffe85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 's', 'your', 'name']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"What's your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "64300864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocalulary\n",
    "vocab = {}\n",
    "vocab = {'<UNK>': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c1b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from dataframe rows (robust to missing columns/values)\n",
    "def build_vocab_from_row(row):\n",
    "    # Row can be a Series; use get with default empty string\n",
    "    q_text = row.get('Question', '') if hasattr(row, 'get') else ''\n",
    "    a_text = row.get('Answer', '') if hasattr(row, 'get') else ''\n",
    "    q_tokens = tokenize(q_text)\n",
    "    a_tokens = tokenize(a_text)\n",
    "    for token in q_tokens + a_tokens:\n",
    "        if token and token not in vocab:\n",
    "            vocab[token] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6cc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "810b6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_indices(text, vocab):\n",
    "    indexed_text = []\n",
    "\n",
    "    for token in tokenize(text):\n",
    "        indexed_text.append(vocab[token])\n",
    "    else:\n",
    "        indexed_text.append(vocab['<UNK>'])\n",
    "    return indexed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "888ec8d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'what'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[127]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtext_to_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is your name?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[126]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mtext_to_indices\u001b[39m\u001b[34m(text, vocab)\u001b[39m\n\u001b[32m      2\u001b[39m indexed_text = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenize(text):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     indexed_text.append(\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      7\u001b[39m     indexed_text.append(vocab[\u001b[33m'\u001b[39m\u001b[33m<UNK>\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mKeyError\u001b[39m: 'what'"
     ]
    }
   ],
   "source": [
    "text_to_indices(\"What is your name?\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae9d046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
